# beyondBERT
11.5기의 beyondBERT의 토론 내용을 정리하는 repository입니다.

## 진행방식
- 논문 당 한명의 호스트가 모임을 진행합니다.
- 호스트는 배정된 논문의 idea 위주로 정리합니다.
- 사전학습으로 참가자는 주차 별 논문에 question을 한개 이상 준비하고, 논문 issue의 thread에 question을 등록합니다.
- thread에 달린 question에 thumb up을 하고, thumb up의 개수가 높은 question 위주로 모임시간에 토론하고 해결합니다.

## 주차 별 논문
### week01
- ice breaking
- 진행 방식 결정
### week02
- [The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives](https://arxiv.org/abs/1909.01380)
- [How multilingual is Multilingual BERT?](https://arxiv.org/abs/1906.01502)
### week03
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
### week04
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)
### week05
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
### week06
- [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324)
- [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)
### week07
- [You Impress Me: Dialogue Generation via Mutual Persona Perception](https://arxiv.org/abs/2004.05388)
- [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637)
### week08
- [ToD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogues](https://arxiv.org/abs/2004.06871)
- [A Simple Language Model for Task-Oriented Dialogue](https://arxiv.org/abs/2005.00796)
### week09
- [ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation](https://arxiv.org/abs/1907.05339)
- [FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178)
### week10
- [PoWER-BERT: Accelerating BERT inference for Classification Tasks](https://arxiv.org/abs/2001.08950)
- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)
### week11
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
